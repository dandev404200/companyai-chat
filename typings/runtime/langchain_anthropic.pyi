"""
Auto-generated type stubs for langchain_anthropic
Generated by runtime inspection
"""

from typing import Any, Dict, List, Optional, Union, Sequence

class AnthropicLLM:
    """Anthropic text completion large language model (legacy LLM).

To use, you should have the environment variable `ANTHROPIC_API_KEY`
set with your API key, or pass it as a named parameter to the constructor.

Example:
    ```python
    from langchain_anthropic import AnthropicLLM

    model = AnthropicLLM(model="claude-sonnet-4-5")
    ```"""
    name: str | None
    cache: langchain_core.caches.BaseCache | bool | None
    verbose: <class 'bool'>
    callbacks: list[langchain_core.callbacks.base.BaseCallbackHandler] | langchain_core.callbacks.base.BaseCallbackManager | None
    tags: list[str] | None
    metadata: dict[str, typing.Any] | None
    custom_get_token_ids: collections.abc.Callable[[str], list[int]] | None
    client: typing.Any
    async_client: typing.Any
    model: <class 'str'>
    max_tokens: <class 'int'>
    temperature: float | None
    top_k: int | None
    top_p: float | None
    streaming: <class 'bool'>
    default_request_timeout: float | None
    max_retries: <class 'int'>
    anthropic_api_url: str | None
    anthropic_api_key: <class 'pydantic.types.SecretStr'>
    HUMAN_PROMPT: str | None
    AI_PROMPT: str | None
    count_tokens: collections.abc.Callable[[str], int] | None
    model_kwargs: dict[str, typing.Any]
    def build_extra(values: dict) -> typing.Any: ...
    def construct(_fields_set: set[str] | None = None, values) -> typing.Self: ...
    def from_orm(obj) -> typing.Self: ...
    def get_lc_namespace() -> list[str]: ...
    def is_lc_serializable() -> <class 'bool'>: ...
    def lc_id() -> list[str]: ...
    def model_construct(_fields_set: set[str] | None = None, values) -> typing.Self: ...
    def model_json_schema(by_alias: bool = True, ref_template: str = #/$defs/{model}, schema_generator: type[GenerateJsonSchema] = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: JsonSchemaMode = validation, union_format: Literal['any_of', 'primitive_type_array'] = any_of) -> dict[str, typing.Any]: ...
    def model_parametrized_name(params: tuple[type[Any], ...]) -> <class 'str'>: ...
    def model_validate(obj, strict: bool | None = None, extra: ExtraValues | None = None, from_attributes: bool | None = None, context: Any | None = None, by_alias: bool | None = None, by_name: bool | None = None) -> typing.Self: ...
    def model_validate_json(json_data: str | bytes | bytearray, strict: bool | None = None, extra: ExtraValues | None = None, context: Any | None = None, by_alias: bool | None = None, by_name: bool | None = None) -> typing.Self: ...
    def model_validate_strings(obj, strict: bool | None = None, extra: ExtraValues | None = None, context: Any | None = None, by_alias: bool | None = None, by_name: bool | None = None) -> typing.Self: ...
    def parse_obj(obj) -> typing.Self: ...
    def raise_warning(values: dict) -> typing.Any: ...
    def schema(by_alias: bool = True, ref_template: str = #/$defs/{model}) -> typing.Dict[str, typing.Any]: ...
    def schema_json(by_alias: bool = True, ref_template: str = #/$defs/{model}, dumps_kwargs) -> <class 'str'>: ...
    def set_verbose(verbose: bool | None) -> <class 'bool'>: ...
    def update_forward_refs(localns) -> <class 'NoneType'>: ...
    def validate(value) -> typing.Self: ...

class ChatAnthropic:
    """Anthropic chat models.

See [Anthropic's docs](https://docs.claude.com/en/docs/about-claude/models/overview)
for a list of the latest models.

Setup:
    Install `langchain-anthropic` and set environment variable `ANTHROPIC_API_KEY`.

    ```bash
    pip install -U langchain-anthropic
    export ANTHROPIC_API_KEY="your-api-key"
    ```

Key init args — completion params:
    model:
        Name of Anthropic model to use. e.g. `'claude-sonnet-4-5-20250929'`.
    temperature:
        Sampling temperature. Ranges from `0.0` to `1.0`.
    max_tokens:
        Max number of tokens to generate.

Key init args — client params:
    timeout:
        Timeout for requests.
    anthropic_proxy:
        Proxy to use for the Anthropic clients, will be used for every API call.
        If not passed in will be read from env var `ANTHROPIC_PROXY`.
    max_retries:
        Max number of retries if a request fails.
    api_key:
        Anthropic API key. If not passed in will be read from env var
        `ANTHROPIC_API_KEY`.
    base_url:
        Base URL for API requests. Only specify if using a proxy or service
        emulator.

See full list of supported init args and their descriptions in the params section.

Instantiate:
    ```python
    from langchain_anthropic import ChatAnthropic

    model = ChatAnthropic(
        model="claude-sonnet-4-5-20250929",
        temperature=0,
        max_tokens=1024,
        timeout=None,
        max_retries=2,
        # api_key="...",
        # base_url="...",
        # other params...
    )
    ```

!!! note
    Any param which is not explicitly supported will be passed directly to the
    `anthropic.Anthropic.messages.create(...)` API every time to the model is
    invoked. For example:

    ```python
    from langchain_anthropic import ChatAnthropic
    import anthropic

    ChatAnthropic(..., extra_headers={}).invoke(...)

    # results in underlying API call of:

    anthropic.Anthropic(..).messages.create(..., extra_headers={})

    # which is also equivalent to:

    ChatAnthropic(...).invoke(..., extra_headers={})
    ```

Invoke:
    ```python
    messages = [
        (
            "system",
            "You are a helpful translator. Translate the user sentence to French.",
        ),
        ("human", "I love programming."),
    ]
    model.invoke(messages)
    ```

    ```python
    AIMessage(
        content="J'aime la programmation.",
        response_metadata={
            "id": "msg_01Trik66aiQ9Z1higrD5XFx3",
            "model": "claude-sonnet-4-5-20250929",
            "stop_reason": "end_turn",
            "stop_sequence": None,
            "usage": {"input_tokens": 25, "output_tokens": 11},
        },
        id="run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0",
        usage_metadata={
            "input_tokens": 25,
            "output_tokens": 11,
            "total_tokens": 36,
        },
    )
    ```

Stream:
    ```python
    for chunk in model.stream(messages):
        print(chunk.text, end="")
    ```

    ```python
    AIMessageChunk(content="J", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
    AIMessageChunk(content="'", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
    AIMessageChunk(content="a", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
    AIMessageChunk(content="ime", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
    AIMessageChunk(content=" la", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
    AIMessageChunk(content=" programm", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
    AIMessageChunk(content="ation", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
    AIMessageChunk(content=".", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
    ```

    ```python
    stream = model.stream(messages)
    full = next(stream)
    for chunk in stream:
        full += chunk
    full
    ```

    ```python
    AIMessageChunk(content="J'aime la programmation.", id="run-b34faef0-882f-4869-a19c-ed2b856e6361")
    ```

Async:
    ```python
    await model.ainvoke(messages)

    # stream:
    # async for chunk in (await model.astream(messages))

    # batch:
    # await model.abatch([messages])
    ```

    ```python
    AIMessage(
        content="J'aime la programmation.",
        response_metadata={
            "id": "msg_01Trik66aiQ9Z1higrD5XFx3",
            "model": "claude-sonnet-4-5-20250929",
            "stop_reason": "end_turn",
            "stop_sequence": None,
            "usage": {"input_tokens": 25, "output_tokens": 11},
        },
        id="run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0",
        usage_metadata={
            "input_tokens": 25,
            "output_tokens": 11,
            "total_tokens": 36,
        },
    )
    ```

Tool calling:
    ```python
    from pydantic import BaseModel, Field


    class GetWeather(BaseModel):
        '''Get the current weather in a given location'''

        location: str = Field(..., description="The city and state, e.g. San Francisco, CA")


    class GetPopulation(BaseModel):
        '''Get the current population in a given location'''

        location: str = Field(..., description="The city and state, e.g. San Francisco, CA")


    model_with_tools = model.bind_tools([GetWeather, GetPopulation])
    ai_msg = model_with_tools.invoke("Which city is hotter today and which is bigger: LA or NY?")
    ai_msg.tool_calls
    ```

    ```python
    [
        {
            "name": "GetWeather",
            "args": {"location": "Los Angeles, CA"},
            "id": "toolu_01KzpPEAgzura7hpBqwHbWdo",
        },
        {
            "name": "GetWeather",
            "args": {"location": "New York, NY"},
            "id": "toolu_01JtgbVGVJbiSwtZk3Uycezx",
        },
        {
            "name": "GetPopulation",
            "args": {"location": "Los Angeles, CA"},
            "id": "toolu_01429aygngesudV9nTbCKGuw",
        },
        {
            "name": "GetPopulation",
            "args": {"location": "New York, NY"},
            "id": "toolu_01JPktyd44tVMeBcPPnFSEJG",
        },
    ]
    ```

    See `ChatAnthropic.bind_tools()` method for more.

Structured output:
    ```python
    from typing import Optional

    from pydantic import BaseModel, Field


    class Joke(BaseModel):
        '''Joke to tell user.'''

        setup: str = Field(description="The setup of the joke")
        punchline: str = Field(description="The punchline to the joke")
        rating: int | None = Field(description="How funny the joke is, from 1 to 10")


    structured_model = model.with_structured_output(Joke)
    structured_model.invoke("Tell me a joke about cats")
    ```

    ```python
    Joke(
        setup="Why was the cat sitting on the computer?",
        punchline="To keep an eye on the mouse!",
        rating=None,
    )
    ```

    See `ChatAnthropic.with_structured_output()` for more.

Image input:
    See [multimodal guides](https://docs.langchain.com/oss/python/langchain/models#multimodal)
    for more detail.

    ```python
    import base64

    import httpx
    from langchain_anthropic import ChatAnthropic
    from langchain_core.messages import HumanMessage

    image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
    image_data = base64.b64encode(httpx.get(image_url).content).decode("utf-8")

    model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
    message = HumanMessage(
        content=[
            {
                "type": "text",
                "text": "Can you highlight the differences between these two images?",
            },
            {
                "type": "image",
                "base64": image_data,
                "mime_type": "image/jpeg",
            },
            {
                "type": "image",
                "url": image_url,
            },
        ],
    )
    ai_msg = model.invoke([message])
    ai_msg.content
    ```

    ```python
    "After examining both images carefully, I can see that they are actually identical."
    ```

    ??? note "Files API"

        You can also pass in files that are managed through Anthropic's
        [Files API](https://docs.claude.com/en/docs/build-with-claude/files):

        ```python
        from langchain_anthropic import ChatAnthropic

        model = ChatAnthropic(
            model="claude-sonnet-4-5-20250929",
            betas=["files-api-2025-04-14"],
        )
        input_message = {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Describe this document.",
                },
                {
                    "type": "image",
                    "id": "file_abc123...",
                },
            ],
        }
        model.invoke([input_message])
        ```

PDF input:
    See [multimodal guides](https://docs.langchain.com/oss/python/langchain/models#multimodal)
    for more detail.

    ```python
    from base64 import b64encode
    from langchain_anthropic import ChatAnthropic
    from langchain_core.messages import HumanMessage
    import requests

    url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"
    data = b64encode(requests.get(url).content).decode()

    model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
    ai_msg = model.invoke(
        [
            HumanMessage(
                [
                    "Summarize this document.",
                    {
                        "type": "file",
                        "mime_type": "application/pdf",
                        "base64": data,
                    },
                ]
            )
        ]
    )
    ai_msg.content
    ```

    ```python
    "This appears to be a simple document..."
    ```

    ??? note "Files API"

        You can also pass in files that are managed through Anthropic's
        [Files API](https://docs.claude.com/en/docs/build-with-claude/files):

        ```python
        from langchain_anthropic import ChatAnthropic

        model = ChatAnthropic(
            model="claude-sonnet-4-5-20250929",
            betas=["files-api-2025-04-14"],
        )
        input_message = {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Describe this document.",
                },
                {
                    "type": "file",
                    "id": "file_abc123...",
                },
            ],
        }
        model.invoke([input_message])
        ```

Extended thinking:
    Certain [Claude models](https://docs.claude.com/en/docs/build-with-claude/extended-thinking#supported-models)
    support an [extended thinking](https://docs.claude.com/en/docs/build-with-claude/extended-thinking)
    feature, which will output the step-by-step reasoning process that led to its
    final answer.

    To use it, specify the `thinking` parameter when initializing `ChatAnthropic`.

    It can also be passed in as a kwarg during invocation.

    You will need to specify a token budget to use this feature. See usage example:

    ```python
    from langchain_anthropic import ChatAnthropic

    model = ChatAnthropic(
        model="claude-sonnet-4-5-20250929",
        max_tokens=5000,
        thinking={"type": "enabled", "budget_tokens": 2000},
    )

    response = model.invoke("What is the cube root of 50.653?")
    response.content
    ```

    ```python
    [
        {
            "signature": "...",
            "thinking": "To find the cube root of 50.653...",
            "type": "thinking",
        },
        {"text": "The cube root of 50.653 is ...", "type": "text"},
    ]
    ```

    !!! warning "Differences in thinking across model versions"
        The Claude Messages API handles thinking differently across Claude Sonnet
        3.7 and Claude 4 models. Refer to [their docs](https://docs.claude.com/en/docs/build-with-claude/extended-thinking#differences-in-thinking-across-model-versions)
        for more info.

Citations:
    Anthropic supports a [citations](https://docs.claude.com/en/docs/build-with-claude/citations)
    feature that lets Claude attach context to its answers based on source
    documents supplied by the user. When [document content blocks](https://docs.claude.com/en/docs/build-with-claude/citations#document-types)
    with `"citations": {"enabled": True}` are included in a query, Claude may
    generate citations in its response.

    ```python
    from langchain_anthropic import ChatAnthropic

    model = ChatAnthropic(model="claude-3-5-haiku-20241022")

    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "document",
                    "source": {
                        "type": "text",
                        "media_type": "text/plain",
                        "data": "The grass is green. The sky is blue.",
                    },
                    "title": "My Document",
                    "context": "This is a trustworthy document.",
                    "citations": {"enabled": True},
                },
                {"type": "text", "text": "What color is the grass and sky?"},
            ],
        }
    ]
    response = model.invoke(messages)
    response.content
    ```

    ```python
    [
        {"text": "Based on the document, ", "type": "text"},
        {
            "text": "the grass is green",
            "type": "text",
            "citations": [
                {
                    "type": "char_location",
                    "cited_text": "The grass is green. ",
                    "document_index": 0,
                    "document_title": "My Document",
                    "start_char_index": 0,
                    "end_char_index": 20,
                }
            ],
        },
        {"text": ", and ", "type": "text"},
        {
            "text": "the sky is blue",
            "type": "text",
            "citations": [
                {
                    "type": "char_location",
                    "cited_text": "The sky is blue.",
                    "document_index": 0,
                    "document_title": "My Document",
                    "start_char_index": 20,
                    "end_char_index": 36,
                }
            ],
        },
        {"text": ".", "type": "text"},
    ]
    ```

Token usage:
    ```python
    ai_msg = model.invoke(messages)
    ai_msg.usage_metadata
    ```

    ```python
    {"input_tokens": 25, "output_tokens": 11, "total_tokens": 36}
    ```

    Message chunks containing token usage will be included during streaming by
    default:

    ```python
    stream = model.stream(messages)
    full = next(stream)
    for chunk in stream:
        full += chunk
    full.usage_metadata
    ```

    ```python
    {"input_tokens": 25, "output_tokens": 11, "total_tokens": 36}
    ```

    These can be disabled by setting `stream_usage=False` in the stream method,
    or by setting `stream_usage=False` when initializing ChatAnthropic.

Prompt caching:
    Prompt caching reduces processing time and costs for repetitive tasks or prompts
    with consistent elements

    !!! note
        Only certain models support prompt caching.
        See the [Claude documentation](https://docs.claude.com/en/docs/build-with-claude/prompt-caching#supported-models)
        for a full list.

    ```python
    from langchain_anthropic import ChatAnthropic

    model = ChatAnthropic(model="claude-sonnet-4-5-20250929")

    messages = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": "Below is some long context:",
                },
                {
                    "type": "text",
                    "text": f"{long_text}",
                    "cache_control": {"type": "ephemeral"},
                },
            ],
        },
        {
            "role": "user",
            "content": "What's that about?",
        },
    ]

    response = model.invoke(messages)
    response.usage_metadata["input_token_details"]
    ```

    ```python
    {"cache_read": 0, "cache_creation": 1458}
    ```

    Alternatively, you may enable prompt caching at invocation time. You may want to
    conditionally cache based on runtime conditions, such as the length of the
    context. Alternatively, this is useful for app-level decisions about what to
    cache.

    ```python
    response = model.invoke(
        messages,
        cache_control={"type": "ephemeral"},
    )
    ```

    ??? note "Extended caching"

        The cache lifetime is 5 minutes by default. If this is too short, you can
        apply one hour caching by setting `ttl` to `'1h'`.

        ```python
        model = ChatAnthropic(
            model="claude-sonnet-4-5-20250929",
        )

        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"{long_text}",
                        "cache_control": {"type": "ephemeral", "ttl": "1h"},
                    },
                ],
            }
        ]

        response = model.invoke(messages)
        ```

        Details of cached token counts will be included on the `InputTokenDetails`
        of response's `usage_metadata`:

        ```python
        response = model.invoke(messages)
        response.usage_metadata
        ```

        ```python
        {
            "input_tokens": 1500,
            "output_tokens": 200,
            "total_tokens": 1700,
            "input_token_details": {
                "cache_read": 0,
                "cache_creation": 1000,
                "ephemeral_1h_input_tokens": 750,
                "ephemeral_5m_input_tokens": 250,
            },
        }
        ```

        See [Claude documentation](https://docs.claude.com/en/docs/build-with-claude/prompt-caching#1-hour-cache-duration-beta)
        for detail.

!!! note title="Extended context windows (beta)"

    Claude Sonnet 4 supports a 1-million token context window, available in beta for
    organizations in usage tier 4 and organizations with custom rate limits.

    ```python
    from langchain_anthropic import ChatAnthropic

    model = ChatAnthropic(
        model="claude-sonnet-4-5-20250929",
        betas=["context-1m-2025-08-07"],  # Enable 1M context beta
    )

    long_document = """
    This is a very long document that would benefit from the extended 1M
    context window...
    [imagine this continues for hundreds of thousands of tokens]
    """

    messages = [
        HumanMessage(f"""
    Please analyze this document and provide a summary:

    {long_document}

    What are the key themes and main conclusions?
    """)
    ]

    response = model.invoke(messages)
    ```

    See [Claude documentation](https://docs.claude.com/en/docs/build-with-claude/context-windows#1m-token-context-window)
    for detail.


!!! note title="Token-efficient tool use (beta)"

    See LangChain [docs](https://docs.langchain.com/oss/python/integrations/chat/anthropic)
    for more detail.

    ```python
    from langchain_anthropic import ChatAnthropic
    from langchain_core.tools import tool

    model = ChatAnthropic(
        model="claude-sonnet-4-5-20250929",
        temperature=0,
        model_kwargs={
            "extra_headers": {
                "anthropic-beta": "token-efficient-tools-2025-02-19"
            }
        }
    )

    @tool
    def get_weather(location: str) -> str:
        """Get the weather at a location."""
        return "It's sunny."

    model_with_tools = model.bind_tools([get_weather])
    response = model_with_tools.invoke(
        "What's the weather in San Francisco?"
    )
    print(response.tool_calls)
    print(f'Total tokens: {response.usage_metadata["total_tokens"]}')
    ```

    ```txt
    [{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'toolu_01HLjQMSb1nWmgevQUtEyz17', 'type': 'tool_call'}]
    Total tokens: 408
    ```

!!! note title="Context management"

    Anthropic supports a context editing feature that will automatically manage the
    model's context window (e.g., by clearing tool results).

    See [Anthropic documentation](https://docs.claude.com/en/docs/build-with-claude/context-editing)
    for details and configuration options.

    ```python
    from langchain_anthropic import ChatAnthropic

    model = ChatAnthropic(
        model="claude-sonnet-4-5-20250929",
        betas=["context-management-2025-06-27"],
        context_management={"edits": [{"type": "clear_tool_uses_20250919"}]},
    )
    model_with_tools = model.bind_tools([{"type": "web_search_20250305", "name": "web_search"}])
    response = model_with_tools.invoke("Search for recent developments in AI")
    ```

!!! note title="Built-in tools"

    See LangChain [docs](https://docs.langchain.com/oss/python/integrations/chat/anthropic#built-in-tools)
    for more detail.

    ??? note "Web search"

        ```python
        from langchain_anthropic import ChatAnthropic

        model = ChatAnthropic(model="claude-3-5-haiku-20241022")

        tool = {
            "type": "web_search_20250305",
            "name": "web_search",
            "max_uses": 3,
        }
        model_with_tools = model.bind_tools([tool])

        response = model_with_tools.invoke("How do I update a web app to TypeScript 5.5?")
        ```

    ??? note "Web fetch (beta)"

        ```python
        from langchain_anthropic import ChatAnthropic

        model = ChatAnthropic(
            model="claude-3-5-haiku-20241022",
            betas=["web-fetch-2025-09-10"],  # Enable web fetch beta
        )

        tool = {
            "type": "web_fetch_20250910",
            "name": "web_fetch",
            "max_uses": 3,
        }
        model_with_tools = model.bind_tools([tool])

        response = model_with_tools.invoke("Please analyze the content at https://example.com/article")
        ```

    ??? note "Code execution"

        ```python
        model = ChatAnthropic(
            model="claude-sonnet-4-5-20250929",
            betas=["code-execution-2025-05-22"],
        )

        tool = {"type": "code_execution_20250522", "name": "code_execution"}
        model_with_tools = model.bind_tools([tool])

        response = model_with_tools.invoke(
            "Calculate the mean and standard deviation of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
        )
        ```

    ??? note "Remote MCP"

        ```python
        from langchain_anthropic import ChatAnthropic

        mcp_servers = [
            {
                "type": "url",
                "url": "https://mcp.deepwiki.com/mcp",
                "name": "deepwiki",
                "tool_configuration": {  # optional configuration
                    "enabled": True,
                    "allowed_tools": ["ask_question"],
                },
                "authorization_token": "PLACEHOLDER",  # optional authorization
            }
        ]

        model = ChatAnthropic(
            model="claude-sonnet-4-5-20250929",
            betas=["mcp-client-2025-04-04"],
            mcp_servers=mcp_servers,
        )

        response = model.invoke(
            "What transport protocols does the 2025-03-26 version of the MCP "
            "spec (modelcontextprotocol/modelcontextprotocol) support?"
        )
        ```

    ??? note "Text editor"

        ```python
        from langchain_anthropic import ChatAnthropic

        model = ChatAnthropic(model="claude-sonnet-4-5-20250929")

        tool = {"type": "text_editor_20250124", "name": "str_replace_editor"}
        model_with_tools = model.bind_tools([tool])

        response = model_with_tools.invoke(
            "There's a syntax error in my primes.py file. Can you help me fix it?"
        )
        print(response.text)
        response.tool_calls
        ```

        ```txt
        I'd be happy to help you fix the syntax error in your primes.py file. First, let's look at the current content of the file to identify the error.

        [{'name': 'str_replace_editor',
        'args': {'command': 'view', 'path': '/repo/primes.py'},
        'id': 'toolu_01VdNgt1YV7kGfj9LFLm6HyQ',
        'type': 'tool_call'}]
        ```

    ??? note "Memory tool"

        ```python
        from langchain_anthropic import ChatAnthropic

        model = ChatAnthropic(
            model="claude-sonnet-4-5-20250929",
            betas=["context-management-2025-06-27"],
        )
        model_with_tools = model.bind_tools([{"type": "memory_20250818", "name": "memory"}])
        response = model_with_tools.invoke("What are my interests?")
        ```

!!! note title="Response metadata"

    ```python
    ai_msg = model.invoke(messages)
    ai_msg.response_metadata
    ```

    ```python
    {
        "id": "msg_013xU6FHEGEq76aP4RgFerVT",
        "model": "claude-sonnet-4-5-20250929",
        "stop_reason": "end_turn",
        "stop_sequence": None,
        "usage": {"input_tokens": 25, "output_tokens": 11},
    }
    ```"""
    name: str | None
    cache: langchain_core.caches.BaseCache | bool | None
    verbose: <class 'bool'>
    callbacks: list[langchain_core.callbacks.base.BaseCallbackHandler] | langchain_core.callbacks.base.BaseCallbackManager | None
    tags: list[str] | None
    metadata: dict[str, typing.Any] | None
    custom_get_token_ids: collections.abc.Callable[[str], list[int]] | None
    rate_limiter: langchain_core.rate_limiters.BaseRateLimiter | None
    disable_streaming: typing.Union[bool, typing.Literal['tool_calling']]
    output_version: str | None
    model: <class 'str'>
    max_tokens: int | None
    temperature: float | None
    top_k: int | None
    top_p: float | None
    default_request_timeout: float | None
    max_retries: <class 'int'>
    stop_sequences: list[str] | None
    anthropic_api_url: str | None
    anthropic_api_key: <class 'pydantic.types.SecretStr'>
    anthropic_proxy: str | None
    default_headers: collections.abc.Mapping[str, str] | None
    betas: list[str] | None
    model_kwargs: dict[str, typing.Any]
    streaming: <class 'bool'>
    stream_usage: <class 'bool'>
    thinking: dict[str, typing.Any] | None
    mcp_servers: list[dict[str, typing.Any]] | None
    context_management: dict[str, typing.Any] | None
    def build_extra(values: dict) -> typing.Any: ...
    def construct(_fields_set: set[str] | None = None, values) -> typing.Self: ...
    def from_orm(obj) -> typing.Self: ...
    def get_lc_namespace() -> list[str]: ...
    def is_lc_serializable() -> <class 'bool'>: ...
    def lc_id() -> list[str]: ...
    def model_construct(_fields_set: set[str] | None = None, values) -> typing.Self: ...
    def model_json_schema(by_alias: bool = True, ref_template: str = #/$defs/{model}, schema_generator: type[GenerateJsonSchema] = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: JsonSchemaMode = validation, union_format: Literal['any_of', 'primitive_type_array'] = any_of) -> dict[str, typing.Any]: ...
    def model_parametrized_name(params: tuple[type[Any], ...]) -> <class 'str'>: ...
    def model_validate(obj, strict: bool | None = None, extra: ExtraValues | None = None, from_attributes: bool | None = None, context: Any | None = None, by_alias: bool | None = None, by_name: bool | None = None) -> typing.Self: ...
    def model_validate_json(json_data: str | bytes | bytearray, strict: bool | None = None, extra: ExtraValues | None = None, context: Any | None = None, by_alias: bool | None = None, by_name: bool | None = None) -> typing.Self: ...
    def model_validate_strings(obj, strict: bool | None = None, extra: ExtraValues | None = None, context: Any | None = None, by_alias: bool | None = None, by_name: bool | None = None) -> typing.Self: ...
    def parse_obj(obj) -> typing.Self: ...
    def schema(by_alias: bool = True, ref_template: str = #/$defs/{model}) -> typing.Dict[str, typing.Any]: ...
    def schema_json(by_alias: bool = True, ref_template: str = #/$defs/{model}, dumps_kwargs) -> <class 'str'>: ...
    def set_default_max_tokens(values: dict[str, Any]) -> typing.Any: ...
    def set_verbose(verbose: bool | None) -> <class 'bool'>: ...
    def update_forward_refs(localns) -> <class 'NoneType'>: ...
    def validate(value) -> typing.Self: ...

def convert_to_anthropic_tool(tool: dict[str, Any] | type | Callable | BaseTool) -> <class 'langchain_anthropic.chat_models.AnthropicTool'>:
    """Convert a tool-like object to an Anthropic tool definition."""
    ...
